import math
from scipy import special
from tqdm import trange
import numpy as np
import matplotlib.pyplot as plt
import env_access
import copy
import os
import torch
import torch.nn as nn
import torch.nn.functional as F

# np.random.seed(0)
class Net(nn.Module):
    def __init__(self, n_feature, n_hidden):
        super(Net, self).__init__()
        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer
        self.out = torch.nn.Linear(n_hidden, 1)   # output layer

    def forward(self, x):
        x = F.relu(self.hidden(x))      # activation function for hidden layer
        x = self.out(x)
        return x


class AccessNode:
    def __init__(self, index, deadline, k, buffer_size, La, node_per_grid, env):
        self.index = index
        self.La = La # feature vector length for action

        self.state = []  # The list of local state at different time steps
        self.action = []  # The list of local actions at different time steps
        self.reward = []  # The list of local reward at different time steps
        self.avgreward = [] # The list of average local reward at different time steps
        self.kHop = []  # The list to record the (state, action) pairs of k-hop neighbors
        self.qFeatureMtx = {}
        self.actionFeatureMtx = {}
        self.q =  {}
        self.params = np.zeros(self.La)
        self.paramsDict = {}

        
        self.k = k
        self.ddl = deadline  # the initial deadline of each packet
        self.node_per_grid = node_per_grid
        self.accessPoints = env.grid_net.find_access(
            i= index//node_per_grid)  # find and cache the access points this node can access
        self.accessNum = len(self.accessPoints)  # the number of access points
        self.actionNum = self.accessNum * self.ddl + 1  # the number of possible actions, action is a tuple (slot, accessPoint)
        # construct a list of possible actions
        self.actionList = [(-1, -1)]  # (-1, -1) is an empty action that does nothing
        for slot in range(self.ddl):
            for a in self.accessPoints:
                self.actionList.append((slot, a))
        self.env = env

        self.buffer_size = buffer_size
        self.Q_Net = None
        self.init_Q_Net()
        self.StateAction_list = []
        self.r_list = []

    def restart(self):
        self.state.clear()
        self.action.clear()
        self.reward.clear()
        self.avgreward.clear()
        self.kHop.clear()

    # initialize the local state (called at the beginning of the training process)
    def initialize_state(self):
        # self.state.append(self.env.observe_state_g_v2(self.index, self.k))  # append this state to state record
        # depth = 0 when node_per_grid>0
        self.state.append(self.env.observe_state_g_v2(self.index, 0))  # append this state to state record
        
        # self.state.append(self.env.observe_state_g(self.index))  
        # self.state.append(self.env.observe_local_state_g_v2(self.index, 0))

        
    def restart_params(self):
        self.params = np.zeros(self.La)

    # At each time, user node observes the state of the whole grid
    def update_state(self):
        # self.state.append(self.env.observe_state_g_v2(self.index, self.k))  # append this state to state record
        # depth = 0 when node_per_grid>0
        self.state.append(self.env.observe_state_g_v2(self.index, 0))  # append this state to state record

        # self.state.append(self.env.observe_local_state_g_v2(self.index, 0))
        # self.state.append(self.env.observe_state_g(self.index))

    # need to call this after the first time step
    def update_k_hop(self):
        # self.kHop.append(self.env.observe_local_state_action_g(self.index))
        self.kHop.append(self.env.observe_state_action_g(self.index))
    
    def update_action(self):
        # get the current state
        curr_State = self.state[-1]
        
        # policy function approximation
        if curr_State not in self.actionFeatureMtx:
            self.actionFeatureMtx[curr_State] = np.random.rand(self.actionNum, self.La)
        # compute the probability vector
        probVec = special.softmax(np.matmul(self.actionFeatureMtx[curr_State], self.params))

        # randomly select an action based on probVec
        currentAction = self.actionList[np.random.choice(a=self.actionNum, p=probVec)]

        self.action.append(currentAction)
        self.env.update_action(self.index, currentAction)

    # oneHopNeighbors is a list of accessNodes
    def update_reward(self):
        currentReward = self.env.observe_reward(self.index)
        self.reward.append(currentReward)

    def update_avgreward(self,_alpha):
        if len(self.avgreward) == 0:
            r_temp = _alpha * self.reward[-1]
        else:
            r_temp = (1-_alpha) * self.avgreward[-1] + _alpha * self.reward[-1]
        self.avgreward.append(r_temp)


 
    # Q-function function approximation
    def init_Q_Net(self):
        n_feature = (1 + node_per_grid*(ddl+1 + (height - 1)*(width - 1)))\
                * len(env.grid_net.find_neighbors(self.index//node_per_grid, self.k))
        n_hidden = min(int(n_feature / 2), 64)
        self.Q_Net = Net(n_feature, n_hidden) 
        self.optimizer = torch.optim.Adam(self.Q_Net.parameters(), lr=1e-4)

    def Q_net_input(self, StateAction_pair):
        max_len = 1000
        tmp_list = list(StateAction_pair)
        result_vector = np.zeros(max_len)
        pointer = 0

        for i in range(len(tmp_list)):
            SA = tmp_list[i]
            state_list = list(SA[0])
            for e in state_list:
                result_vector[pointer] = e
                pointer += 1
            action_list = list(SA[1])
            for j in range(len(action_list)):
                act = action_list[j]
                loc, dest = act
                result_vector[pointer + loc + 1] = 1
                pointer += (ddl + 1)
                if dest != -1:
                    result_vector[pointer + dest] = 1
                pointer += (height - 1)*(width - 1)
                
        return result_vector[:pointer]

    def calc_q(self, StateAction_pair):
        Q_net_input_tensor = torch.tensor(self.Q_net_input(StateAction_pair), dtype=torch.float)
        Q_value = self.Q_Net(Q_net_input_tensor)
        return Q_value.detach().numpy()

    # kHopNeighbors is a list of accessNodes, alpha is learning rate
    def update_q_model(self):        
        self.StateAction_list.append(self.Q_net_input(self.kHop[-2]))
        self.r_list.append(self.reward[-1] - self.avgreward[-1])
        if len(self.StateAction_list) < self.buffer_size:
            return
        self.StateAction_list.append(self.Q_net_input(self.kHop[-1]))

        evaluate_list = copy.deepcopy(self.StateAction_list[1:])
        eva_batch = torch.tensor(evaluate_list, dtype=torch.float)
        with torch.no_grad():
            td_target_list = self.Q_Net(eva_batch).numpy()
        for t in range(self.buffer_size):
            td_target_list[t] =td_target_list[t] + self.r_list[t]

        

        sa_batch = torch.tensor(self.StateAction_list[:self.buffer_size], dtype=torch.float)
        td_target_temp = torch.tensor(td_target_list, dtype=torch.float)

        loss = F.smooth_l1_loss(td_target_temp.detach(), self.Q_Net(sa_batch))  
        # perform the Q value update
        self.optimizer.zero_grad()   # clear gradients for next train
        loss.backward()         # backpropagation, compute gradients
        self.optimizer.step()        # apply gradients

        self.StateAction_list = []
        self.r_list = []

    # '''
    

    

    # eta is the learning rate
    def update_params(self, _beta, avg_Q = 'None'):
        
        # curr_State_localAction = (self.state[-2], self.action[-2])
        last_State = self.state[-2] 
        last_StateAction = self.kHop[-2]    

        if avg_Q == 'None':
            avg_Q = self.calc_q(last_StateAction)

        '''
        params = self.paramsDict.get(last_State, np.zeros(self.actionNum))
        probVec = special.softmax(params)
        grad = -probVec
        actionIndex = self.actionList.index(self.action[-2])  # get the index of currentAction
        grad[actionIndex] += 1.0
        self.paramsDict[last_State] = params + _beta * avg_Q * grad    
        '''


        # '''
        # policy function approximation
        param_temp = special.softmax(np.matmul(self.actionFeatureMtx[last_State], self.params))
        param_temp2 = np.zeros(self.La)
        for i in range(self.actionNum):
            param_temp2 += param_temp[i] * self.actionFeatureMtx[last_State][i,:]
        _action_index = self.actionList.index(self.action[-2])
        _grad = self.actionFeatureMtx[last_State][_action_index,:] - param_temp2 
        self.params += _beta * _grad * avg_Q
        # '''

    def total_reward(self):
        totalReward = np.sum(self.reward) / len(self.reward)
        return totalReward

    def update_action_benchmark(self, benchmark_policy):
        actProb = benchmark_policy[0]
        flagAct = np.random.binomial(1, actProb)  # should I send out a packet?
        if flagAct == 0:
            self.action.append((-1, -1))
            self.env.update_action(self.index, (-1, -1))
            return
        # find the packet with the earliest ddl
        benchSlot = -1
        local_state_temp = env.observe_local_state_g(self.index, 0)[self.index % self.node_per_grid]
        for d in range(self.ddl):
            if local_state_temp[d] > 0:
                benchSlot = d
                break
        if benchSlot == -1:
            self.action.append((-1, -1))
            self.env.update_action(self.index, (-1, -1))
            return
        # select the access point to send to
        benchProb = benchmark_policy[1:]
        benchAccessPoint = self.accessPoints[np.random.choice(a=self.accessNum, p=benchProb)]
        self.action.append((benchSlot, benchAccessPoint))
        self.env.update_action(self.index, (benchSlot, benchAccessPoint))
        return        

def update_qValue_out(neighbor_grids):
    avg_Q_temp = 0.0
    for _grid in neighbor_grids:
        for _node in _grid:
            last_StateAction = _node.kHop[-2]
            avg_Q_temp += _node.calc_q(last_StateAction)
    return avg_Q_temp

def eval_benchmark(node_list, rounds, T, act_prob, env):
    totalRewardSum = 0.0
    benchmarkPolicyList = []
    for i in range(gridNum):
        accessPoints = env.grid_net.find_access(i)
        accessPointsNum = len(accessPoints)
        benchmarkPolicy = np.zeros(accessPointsNum + 1)
        totalSum = 0.0
        for j in range(accessPointsNum):
            tmp = 100 * env.grid_net.transmitProb[accessPoints[j]] \
                / env.grid_net.serviceNum[accessPoints[j]] \
                    / node_per_grid
            totalSum += tmp
            benchmarkPolicy[j + 1] = tmp 
        benchmarkPolicy[1:] = benchmarkPolicy[1:]/totalSum 
        benchmarkPolicy[0] = act_prob
        benchmarkPolicyList.append(benchmarkPolicy)

    for _ in range(1, rounds+1):
        env.initialize()
        for i in range(nodeNum):
            node_list[i].restart()
            node_list[i].initialize_state()

        for i in range(nodeNum):
            node_list[i].update_action_benchmark(benchmarkPolicyList[i//node_per_grid])
    
        for _ in range(1, T + 1):
            env.generate_reward()
            for i in range(nodeNum):
                node_list[i].update_reward()
            env.step()
            for i in range(nodeNum):
                node_list[i].update_state()
            for i in range(nodeNum):
                node_list[i].update_action_benchmark(benchmarkPolicyList[i//node_per_grid])
        # compute the total reward
        averageReward = 0.0
        for i in range(nodeNum):
            averageReward += node_list[i].total_reward()
        averageReward /= nodeNum
        totalRewardSum += averageReward
    return totalRewardSum / rounds

# do not update Q when evaluating a policy
def eval_policy(node_list, rounds, T, env):
    totalRewardSum = 0.0
    
    for _ in range(rounds):
        env.initialize()
        for i in range(nodeNum):
            node_list[i].restart()
            node_list[i].initialize_state()
        for i in range(nodeNum):
            node_list[i].update_action()
        env.generate_reward()
        for i in range(nodeNum):
            node_list[i].update_reward()

        for _ in range(1, T + 1):
            env.step()
            for i in range(nodeNum):
                node_list[i].update_state()
            for i in range(nodeNum):
                node_list[i].update_action()
            env.generate_reward()
            for i in range(nodeNum):
                node_list[i].update_reward()
        # compute the total reward
        averageReward = 0.0
        for i in range(nodeNum):
            averageReward += node_list[i].total_reward()
        averageReward /= nodeNum
        totalRewardSum += averageReward
    return totalRewardSum/rounds 


if __name__ == "__main__":
    k = 1
    ddl = 1
    height = 2
    width = 2
    node_per_grid = 2 # fixed 1
    gridNum = height * width
    nodeNum = gridNum * node_per_grid
    env = env_access.AccessGridEnv(height=height, width=width, k=k, node_per_grid=node_per_grid, ddl = ddl)
    
    # action-value function approximation feature vector dimension
    La = 12 #12 16
    Ep_Num = 1
    T = 1000000 #400000, 50000
    evalInterval = 2000 #2000 10000    # evaluate the policy every evalInterval rounds (outer loop)
    restartInterval = 500  #100 500 2000

    # Ep_Num = 20000
    # T = 10
    # evalInterval = 2000
    # restartInterval = 100

    _scale = 1
    _alpha = lambda t: 1 / (t+1) ** 0.65 * _scale
    _beta = lambda t: 1 / (t+1) ** 0.85 * _scale     
    # _alpha = lambda t: 1 / ((t+1) % 1000 + 1)** 0.5
    # _beta = lambda t: 5 / ((t+1) % 100 + 1) ** 0.5   
    # _alpha = lambda t: 1 / (t+1) ** 0.65 
    # _beta = lambda t: 1 / (t+1) ** 0.85 * 5  
    
    accessNodeList = []
    for i in range(nodeNum):
        accessNodeList.append(\
            AccessNode(index=i, deadline = ddl, k=k, buffer_size=1, La = La, node_per_grid=node_per_grid, env=env))

    # Performance evaluation metrics
    policyRewardList = []
    # experi_avg_reward  = np.zeros(T)  
    temp_avg_reward0 = []
    
    prob_state = np.zeros((nodeNum,T))

    script_dir = os.path.dirname(__file__)
    # with open(script_dir+'\\data/my-PFA-QFA-Tabular-Access-{}-{}-{}-{}-{}.txt'.format(height, width, k, node_per_grid, ddl), 'w') as f:  # used to check the progress of learning
    with open(script_dir+\
        '\\data/my-PNN-QFA3-Tabular-Access-{}-{}-{}-{}-{}.txt'.format(height, width, k, node_per_grid, ddl), 'w') as f:  # used to check the progress of learning
        # first erase the file
        f.seek(0)
        f.truncate() 

    # Actor-critic
    for e in range(Ep_Num):
        # temp_experi_avg_reward = np.zeros((nodeNum,T))
        policy_record = []
        env.initialize()
        for i in range(nodeNum):
            accessNodeList[i].restart()
            # accessNodeList[i].restart_params()
            accessNodeList[i].initialize_state()
        for i in range(nodeNum):
            accessNodeList[i].update_action()
        for i in range(nodeNum):
            accessNodeList[i].update_k_hop()



        for t in trange(1,T+1): 
            env.generate_reward()
            for i in range(nodeNum):
                accessNodeList[i].update_reward()
            env.step()
            for i in range(nodeNum):
                accessNodeList[i].update_state()
            for i in range(nodeNum):
                accessNodeList[i].update_action()
            for i in range(nodeNum):
                accessNodeList[i].update_k_hop()


            avg_Q = []
            if t == 1:
                avg_Q = [0] * gridNum
            else:
                for j in range(gridNum):
                    neighbor_grids = []
                    for a in env.grid_net.find_neighbors(j, k):
                        _temp = []
                        for b in range(node_per_grid):
                            _temp.append(accessNodeList[a*node_per_grid+b])
                        neighbor_grids.append(_temp)
                    avg_Q.append(update_qValue_out(neighbor_grids)/nodeNum)

            for i in range(nodeNum):           
                accessNodeList[i].update_avgreward(1.0 / math.sqrt(t % restartInterval + 1))
                # Q-function function approximation 
                accessNodeList[i].update_q_model()
                # accessNodeList[i].update_q_params(1.0 / math.sqrt(t % restartInterval + 1))
                # accessNodeList[i].update_q(1.0 / math.sqrt(t % restartInterval + 1))
                
                # accessNodeList[i].update_avgreward(_alpha(t))
                # accessNodeList[i].update_q(_alpha(t))
            
            for i in range(nodeNum):
                # accessNodeList[i].update_params(5.0 / (t % restartInterval + 1) ** 0.5,\
                #     avg_Q[i//node_per_grid])
                accessNodeList[i].update_params(1.0 / (t % restartInterval + 1) ** 0.85,\
                    avg_Q[i//node_per_grid])
                # accessNodeList[i].update_params(_beta(t), avg_Q[i//node_per_grid])

            '''
            # test start-------------------------
            state_temp = accessNodeList[0].state[0]
            for i in range(nodeNum):
                prob_temp = special.softmax(accessNodeList[i].paramsDict.get(state_temp))
                # # policy function approximation
                # prob_temp = special.softmax(np.matmul(accessNodeList[i].actionFeatureMtx[state_temp], \
                #     accessNodeList[i].params))
                prob_state[i,t-1] = prob_temp[0]
            print(prob_state[:,t-1])
            # test end------------------------- 
            '''

            if t % evalInterval == evalInterval - 1 or t == 1:
                temp_policy = []
                for i in range(nodeNum):
                    # temp_policy.append(accessNodeList[i].paramsDict)
                    # # policy function approximation
                    temp_policy.append(accessNodeList[i].params)
                policy_record.append(copy.deepcopy(temp_policy))

        

    #     for i in range(nodeNum):
    #         if t == 1:
    #             temp_experi_avg_reward[i,t-1] = accessNodeList[i].reward[-1]
    #         else:
    #             temp_experi_avg_reward[i,t-1] = ((t-1)*temp_experi_avg_reward[i,t-2]+accessNodeList[i].reward[-1])/t

    #     temp_experi_avg_reward = np.sum(temp_experi_avg_reward, axis = 0)/ nodeNum
    #     experi_avg_reward = experi_avg_reward + temp_experi_avg_reward
    # experi_avg_reward = experi_avg_reward / Ep_Num

    # Benchmark Aloha policy
    bestBenchmark = 0.0
    bestBenchmarkProb = 0.0
    for i in range(20):
        tmp = eval_benchmark(node_list=accessNodeList, rounds=1, T=3000, act_prob=i / 20.0, env=env)
        if tmp > bestBenchmark:
            bestBenchmark = tmp
            bestBenchmarkProb = i / 20.0

 
    for _policy in policy_record:
        for i in range(nodeNum):
            # accessNodeList[i].paramsDict = _policy[i]
            # # policy function approximation
            accessNodeList[i].params = _policy[i]
        temp = eval_policy(node_list=accessNodeList, rounds=1, T=3000,  env=env)
        temp_avg_reward0.append(temp)
        with open(script_dir+\
            '\\data/my-PNN-QFA3-Tabular-Access-{}-{}-{}-{}-{}.txt'.format(height, width, k, node_per_grid, ddl), 'a') as f:
                f.write("%f\n" % temp)

   
    '''
   # test start-------------------------
    fig1, ax1 = plt.subplots()
    for i in range(nodeNum):
        plt.plot(np.arange(T),prob_state[i,:])
    plt.show()
    

    # softmax update aciton
    for state_temp in list(accessNodeList[0].actionFeatureMtx.keys()):
        prob_state=np.zeros(nodeNum)
        for i in range(nodeNum):
            # prob_temp = special.softmax(accessNodeList[i].paramsDict.get(state_temp))
            # policy function approximation
            prob_temp = special.softmax(np.matmul(accessNodeList[i].actionFeatureMtx[state_temp], \
                accessNodeList[i].params))
            prob_state[i] = prob_temp[0]
        print(state_temp, prob_state)
    # test end-------------------------
    '''

    # temp_avg_reward = np.sum(np.reshape(temp_avg_reward0, [Ep_Num, (T+1) // evalInterval]), axis = 0) / Ep_Num
    fig2, ax2 = plt.subplots()
    lam = np.linspace(0, (len(temp_avg_reward0) - 1) * evalInterval, len(temp_avg_reward0))
    ax2.plot(lam, temp_avg_reward0, linewidth=1, label='Scalable Actor-critic')
    ax2.plot(lam, bestBenchmark*np.ones(len(temp_avg_reward0)), label='Benchmark ALOHA')
    ax2.legend()
    plt.title("Simulated Average Return (\mu)")
    plt.xlabel('iteration')
    plt.savefig(script_dir+\
        "\\data/my-PNN-QFA3-Tabular-Access-{}-{}-{}-{}-{}.jpg".format(height, width, k, node_per_grid,ddl))
    plt.show()